1. Super- vised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds.
2. We find that the proposed model of tokenization provides an improvement in the performance of text classifi- cation with a simple LSTM classifier. 
3. This re- sults in improved performance for sentiment anal- ysis tasks on Japanese and English datasets and Chinese datasets with a larger cache. We find that the proposed model of tokenization provides an improvement in the performance of text classifi- cation with a simple LSTM classifier.
4. Twitter数据集F1 67.75 https://www.kaggle.com/c/ twitter-sentiment-analysis2
5. In recent years, lots of works have been done to solve text classification problems, but just a few of them have explored the explainability of their systems (Camburu et al., 2018; Ouyang et al., 2018). Ribeiro et al. (2016) try to identify an inter- pretable model over the interpretable representa- tion that is locally faithful to the classifier. 
6. To achieve these goals, in this paper, we pro- pose a novel generative explanation framework for text classification, where our model is capable of not only providing the classification predictions but also generating fine-grained information as ex- planations for decisions. The novel idea behind our hybrid generative-discriminative method is to explicitly capture the fine-grained information in- ferred from raw texts, utilizing the information to help interpret the predicted classification results and improve the overall performance.
7. In this paper, we confirm that these models are useful for text classification when the number of labeled instances is small, but demonstrate that fine-tuning to in-domain data is also of critical importance. 
8. Inherent problems in data emerge in a trained model in several ways. Model explanations can show that the model is not inline with human judg- ment or domain expertise. A canonical example is model unfairness, which stems from biases in the training data. 
9. Our ap- proach relies on re-using model parameters trained at upper levels in the taxonomy and fine-tuning them for classifying categories at lower levels.
10. One critical issue is that the number of local classifiers depends on the size of the label hierarchy, making local approaches in- feasible to scale.
11. Most of the current effective methods for text classification task are based on large-scale la- beled data and a great number of parame- ters, but when the supervised training data are few and difficult to be collected, these mod- els are not available.
12. The dominant text classification models in deep learning require a considerable amount of labeled data to learn a large num- ber of parameters. However, such methods may have difficulty in learning the semantic space in the case that only few data are available. 
13. However, it is not always realistic to assume that example labels are clean. Humans make mistakes and, depending on the complexity of the task, there may be disagreement even among expert la- belers.
14. 1 http://nlp.stanford.edu/sentiment/  2 http://cogcomp.cs.illinois.edu/Data/QA/QC/ 3.http://www.di.unipi.it/ ̃gulli/AG_ corpus_of_news_articles.html
15. In the future, we plan to investigate broader applications like summarizaion, translation, question answering, etc.
16. As one of the most fundamental problems in ma- chine learning, automatic classification has been widely studied in several domains. However, many approaches, proven to be effective in tradi- tional classification tasks, cannot catch up with a dynamic and open environment where new classes can emerge after the learning stage (Romera- Paredes and Torr, 2015). For example, the number of topics on social media is growing rapidly, and the classification models are required to recognise the text of the new topics using only general in- formation (e.g., descriptions of the topics) since labelled training instances are unfeasible to ob- tain for each new topic (Lee et al., 2011). This scenario holds in many real-world domains such as object recognition and medical diagnosis (Xian et al., 2017; World Health Organization, 1996).
17. We also find that our architecture is insensitive to win- dow size and enjoy a better robustness.
18. In many domains, however, labeled data is difficult to obtain. This might be due to annotation cost, or simply because there are no available in- stances to annotate before the system has to make the next classification decision.
19. Both methods promote rationale-like features in the input to improve the classification outcome. 
20. A recent strong performer in this line of research is BERT (Devlin et al., 2018), a multi-layer attention-based neural model that is pre-trained on large amounts of plain text and then fine tuned to specific tasks. Systems using BERT have achieved state-of-the-art performance on var- ious NLP tasks, including short text classification.
21. They showed improved accuracy com- pared to non-rationale-augmented CNNs as well as to rationale-augmented SVM models. Our main contributions relative to that work are the introduc- tion of the joint-learning technique, which learns text classification and rationale words identifica- tion concurrently, and the adaptation of the en- tire approach to BERT. The ability of our model to identify rationale spans within sentences also seems useful for interpretability.
22. Our model ProSeqo uses dynamic re- current projections without the need to store or look up any pre-trained embeddings.
23. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks.
24. 意图识别数据集：SWDA MRDA ATIS SNIPS：ProSeqo: Projection Sequence Networks for On-Device Text Classification
25. This is significant improvement given ProSeqo’s small network size, and the fact that we used the same architecture and parameters across all seven NLP tasks, while prior work tuned depending on the task and dataset.
26. Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes.
27. We propose to induce the class-level repre- sentations from support sets to deal with sample- wise diversity in few-shot learning tasks. 
28. We provide datasets for studying three aspects of 0SHOT-TC: topic categorization, emo- tion detection, and situation frame detection – an event level recognition problem. For each dataset, we have standard split for train, dev, and test, and standard separation of seen and unseen classes.
29. However, these methods cannot capture the semantic rela- tions (e.g., entity relations) and rely heavily on the number of training data. Clearly, lacking of train- ing data is still a key bottleneck that prohibits them from successful practical applications. ---"Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification"
30. With the rapid development of online social me- dia and e-commerce, short texts, such as online news, queries, reviews, tweets, are increasingly widespread on the Internet (Song et al., 2014). Short text classification can be widely applied in many domains, ranging from sentiment analysis to news tagging/categorization and query intent classification.
31. n many practical scenarios, the labeled data is scarce, while human labeling is time-consuming and may require expert knowl- edge 
32. We propose a new recurrent convolutional model for text classification by discussing the shortcomings of max pooling operation and also the strength and weakness of convolu- tion operation.
33. Twitter数据集http://www.cs.cmu.edu/˜yww/data/petpeeves.zip
34. Since deep learning models always contain huge number of parameters, they require a large volume of labeled corpus to learn a good document representation. However, in many situations, it is difficult to construct large training sets because acquiring manually labeled documents is very expen- sive. In this case, the performance of such models would be limited due to the lack of training data.
35. onsist of product and movie reviews in 16 different domains. The data in each do- main is randomly split into training set, development set and test set according to the proportion of 70%, 10%, 20%. Statis- tics of the 16 datasets are listed in Table 1. Multi-Domain Sentiment Classification Based on Domain-Aware Embedding and Attention.




    
    