## I Love Papers

### Introduction
1. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds.
2. We find that the proposed model of tokenization provides an improvement in the performance of text classification with a simple LSTM classifier. 
3. This results in improved performance for sentiment analysis tasks on Japanese and English datasets and Chinese datasets with a larger cache. We find that the proposed model of tokenization provides an improvement in the performance of text classification with a simple LSTM classifier.
5. In recent years, lots of works have been done to solve text classification problems, but just a few of them have explored the explainability of their systems (Camburu et al., 2018; Ouyang et al., 2018). Ribeiro et al. (2016) try to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. 
6. To achieve these goals, in this paper, we propose a novel generative explanation framework for text classification, where our model is capable of not only providing the classification predictions but also generating fine-grained information as explanations for decisions. The novel idea behind our hybrid generative-discriminative method is to explicitly capture the fine-grained information inferred from raw texts, utilizing the information to help interpret the predicted classification results and improve the overall performance.
7. In this paper, we confirm that these models are useful for text classification when the number of labeled instances is small, but demonstrate that fine-tuning to in-domain data is also of critical importance. 
8. Inherent problems in data emerge in a trained model in several ways. Model explanations can show that the model is not inline with human judgment or domain expertise. A canonical example is model unfairness, which stems from biases in the training data. 
9. Our approach relies on re-using model parameters trained at upper levels in the taxonomy and fine-tuning them for classifying categories at lower levels.
10. One critical issue is that the number of local classifiers depends on the size of the label hierarchy, making local approaches infeasible to scale.
11. Most of the current effective methods for text classification task are based on large-scale labeled data and a great number of parameters, but when the supervised training data are few and difficult to be collected, these models are not available.
12. The dominant text classification models in deep learning require a considerable amount of labeled data to learn a large number of parameters. However, such methods may have difficulty in learning the semantic space in the case that only few data are available. 
13. However, it is not always realistic to assume that example labels are clean. Humans make mistakes and, depending on the complexity of the task, there may be disagreement even among expert labelers.
15. In the future, we plan to investigate broader applications like summarizaion, translation, question answering, etc.
16. As one of the most fundamental problems in machine learning, automatic classification has been widely studied in several domains. However, many approaches, proven to be effective in traditional classification tasks, cannot catch up with a dynamic and open environment where new classes can emerge after the learning stage (RomeraParedes and Torr, 2015). For example, the number of topics on social media is growing rapidly, and the classification models are required to recognise the text of the new topics using only general information (e.g., descriptions of the topics) since labelled training instances are unfeasible to obtain for each new topic (Lee et al., 2011). This scenario holds in many real-world domains such as object recognition and medical diagnosis (Xian et al., 2017; World Health Organization, 1996).
17. We also find that our architecture is insensitive to window size and enjoy a better robustness.
18. In many domains, however, labeled data is difficult to obtain. This might be due to annotation cost, or simply because there are no available instances to annotate before the system has to make the next classification decision.
19. Both methods promote rationale-like features in the input to improve the classification outcome. 
20. A recent strong performer in this line of research is BERT (Devlin et al., 2018), a multi-layer attention-based neural model that is pre-trained on large amounts of plain text and then fine tuned to specific tasks. Systems using BERT have achieved state-of-the-art performance on various NLP tasks, including short text classification.
21. They showed improved accuracy compared to non-rationale-augmented CNNs as well as to rationale-augmented SVM models. Our main contributions relative to that work are the introduction of the joint-learning technique, which learns text classification and rationale words identification concurrently, and the adaptation of the entire approach to BERT. The ability of our model to identify rationale spans within sentences also seems useful for interpretability.
22. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings.
23. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks.
25. This is significant improvement given ProSeqo’s small network size, and the fact that we used the same architecture and parameters across all seven NLP tasks, while prior work tuned depending on the task and dataset.
26. Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes.
27. We propose to induce the class-level representations from support sets to deal with samplewise diversity in few-shot learning tasks. 
28. We provide datasets for studying three aspects of 0SHOT-TC: topic categorization, emotion detection, and situation frame detection – an event level recognition problem. For each dataset, we have standard split for train, dev, and test, and standard separation of seen and unseen classes.
29. However, these methods cannot capture the semantic relations (e.g., entity relations) and rely heavily on the number of training data. Clearly, lacking of training data is still a key bottleneck that prohibits them from successful practical applications. ---"Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification"
30. With the rapid development of online social media and e-commerce, short texts, such as online news, queries, reviews, tweets, are increasingly widespread on the Internet (Song et al., 2014). Short text classification can be widely applied in many domains, ranging from sentiment analysis to news tagging/categorization and query intent classification.
31. n many practical scenarios, the labeled data is scarce, while human labeling is time-consuming and may require expert knowledge 
32. We propose a new recurrent convolutional model for text classification by discussing the shortcomings of max pooling operation and also the strength and weakness of convolution operation.
34. Since deep learning models always contain huge number of parameters, they require a large volume of labeled corpus to learn a good document representation. However, in many situations, it is difficult to construct large training sets because acquiring manually labeled documents is very expensive. In this case, the performance of such models would be limited due to the lack of training data.
35. onsist of product and movie reviews in 16 different domains. The data in each domain is randomly split into training set, development set and test set according to the proportion of 70%, 10%, 20%. Statistics of the 16 datasets are listed in Table 1. Multi-Domain Sentiment Classification Based on Domain-Aware Embedding and Attention.
36. Text classification is a fundamental text mining task including multi-class classification and multilabel classification. The former only assigns one label to the given document, while the latter classifies one document into different topics. In this paper, we focus on multi-label text classification (MLTC) because it has become one of the core tasks in natural language processing and has been widely applied in topic recognition.
37. CNN: However, CNNs which were first proposed for images, face many crucial challenges in the context of text processing, namely in their elementary blocks: convolution filters and max pooling. These challenges have largely been overlooked by the most existing CNN models proposed for text classification. "Sequential Learning of Convolutional Features for Effective Text Classification"
38. Frequency based feature selection techniques like Bag-ofWords (BoW), Term frequency-Inverse document frequency (TF-IDF) have been used and the features are trained using machine learning classifiers like Logistic Regression (LR) or Naive Bayes (NB). These approaches provided strong baselines for text classification.
39. However, sequential patterns and semantic structure between words play a crucial role in deciding the category of a text. Traditional lexicon approaches fail to capture such complexities. Since the success of deep learning, neural network architectures have outperformed traditional methods like BoW and count based feature selection techniques.
40. Traditional text classifiers are limited to predicting over a fixed set of labels. However, in many real-world applications the label set is frequently changing. For example, in intent classification, new intents may be added over time while others are removed. "Metric Learning for Dynamic Text Classification"
41. We propose to address the problem of dynamic text classification by replacing the traditional, fixed-size output layer with a learned, semantically meaningful metric space. "Metric Learning for Dynamic Text Classification"
42. In recent years, many successful deep learning models have been widely applied to this task. Mainstream deep learning methods usually contain two im- portant components: representation learning and feature ag- gregation. 


### Related Work
1. One of the main reasons our approach improves the model in the original task is that the model is now more robust thanks to the reinforcement provided to the model builder through attributions. From a fairness angle, our technique shares similarities with adversarial training (Zhang et al., 2018a; Madras et al., 2018) in asking the model to optimize for an additional objective that transitively unbiases the classifier. "Incorporating Priors with Feature Attribution on Text Classification"
2. NeuralClassifier reimplements a very large number of the state-of-the-art text encoders, including FastText (Joulin et al., 2016), TextCNN (Kim, 2014), TextRNN (Liu et al., 2016), RCNN (Lai et al., 2015) , VDCNN (Conneau et al., 2016), DPCNN (Johnson and Zhang, 2017) , AttentiveConvNet (Yin and Schu ̈tze, 2017), DRNN (Wang, 2018), Transformer encoder (Vaswani et al., 2017), Star-Transformer encoder (Guo et al., 2019).
3. TF-IDF (task-agnostic) This algorithm views the vocabulary selection as a retrieval problem (Ramos et al., 2003), where term frequency is viewed as the word frequency and document frequency is viewed as the number of sentences where such word appears. 
4. However, as the order in similarity between SVM and XGBoost is less stable, entropy cannot be the sole cause. 
5. Our results show that different approaches can sometimes lead to very different important features, but there exist some consistent patterns between models and methods. For instance, deep learning models tend to generate diverse important features that are different from traditional models; post-hoc methods lead to more similar important features than built-in methods.

### Experimental Configurations
1. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task.
2. We did study more complicated relationships between ai and bj with multilayer perceptrons, but observed no further improvement on the heldout data.
3. We tried to apply the F(:) function on our hidden states before computing eij and it did not further help our models.
4. We also further modeled the interaction by feeding the tuples into feedforward neural networks and addeds the top layer hidden states to the above concatenation.
5. We found that it does not further help the inference accuracy on the held out dataset.
6. We apply a grid search for hyper-parameters: the learning rate is tuned amongst {0.05, 0.01, 0.005, 0.001}, the coefficient of L2 normalization is searched in {10−5, 10−4, · · · , 101, 102}, and the dropout ratio is tuned in {0.0, 0.1, · · · , 0.8} for NFM, GC-MC, and KGAT. Besides, we employ the node dropout technique for GC-MC and KGAT, where the ratio is searched in {0.0, 0.1, · · · , 0.8}. We select the best parameters depending on the performance of validation data.    
7. Each experiment is repeated 3 times, and the average performance is reported.
8. The settings of hyper-parameters are determined by optimizing AUC on a validation set.
9. We randomly split the dataset into 10919/1373/1356 pairs for train/dev/test set. The distribution of the overall rating scores within this corpus is shown in Table 1. 
10. we use 300-dimensional word2vec [Mikolov et al., 2013] vectors to initialize word representations. 
11. 70% of the documents of each class were randomly selected for training, and the remaining 30% were used as a testing set. "Integrating Semantic Knowledge to Tackle Zero-shot Text Classification"
12. These configurations are set on the validation set held out by 10% from the training data. If not specified, the same configurations are used in all the datasets. "Adaptive Convolution for Text Classification"
13. No matter which method we use, important features from SVM and XGBoost are more similar with each other, than with deep learning models (Figure 2). "Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification"
14. For all the tasks, we use 20% of the dataset as the test set. For SVM and XGBoost, we use cross validation on the other 80% to tune hyperparameters. For LSTM with attention and BERT, we use 10% of the dataset as a validation set, and choose the best hyperparameters based on the validation performance. "Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification"
15. For each dataset, we randomly split the full training corpus into training and validation set, where the validation size is the same as the corresponding test size. Then the validation set is fixed for all models for fair comparison. 
16. The convolution layer had three filter sizes [2, 3, 4] with 50 filters for each size, while the intermediate fully-connected layer had 150 units. The activation functions of the filters and the fully-connected layers are ReLU (except the softmax at the output layer). The models were implemented using Keras and trained with Adam optimizer. The macro-average F1 are 0.90 and 0.94 for the Amazon and the ArXiv datasets, respectively. Overall, the ArXiv appears to be an easier task as it is likely solvable by looking at individual keywords. "Human-grounded Evaluations of Explanation Methods for Text Classification"
17. SVM: SVM classifiers using TF-IDF features and LDA features (Blei et al., 2003), are denoted as SVM+TFIDF and SVM+LDA, respectively. "Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification "
18. CNN: CNN (Kim, 2014) with 2 variants: 1) CNN-rand, whose word embeddings are randomly initialized, and 2) CNN-pretrain, whose word embeddings are pre-trained with Wikipedia Corpus. "Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification "
19. LSTM: LSTM (Liu et al., 2016) with and without pre-trained word embeddings, named LSTMrand and LSTM-pretrain, respectively. "Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification"
20. RF is an ensemble learning method that grows a multitude of randomized, uncorrelated decision trees (Breiman, 2001). 
21. SVM are discriminative classifiers, fitting a margin-maximizing hyperplane between classes. They were initially developed as binary linear classifiers (Cortes & Vapnik, 1995), but can be extended to non-linear problems of higher dimensionality through the use of kernels that can accommodate any functional form (Scholkopf & Smola, 2001). 


### Dataset and Hyperparameters Setting
1. Number of samples are 95,692 / 32,128 / 31,866 in the train/dev/test sets respectively. "Incorporating Priors with Feature Attribution on Text Classification"
2. The data corresponding to each category was randomly split into 85% training and 15% validation instances. We restrict the documents in the dataset to a maximum of 100 words. "Hierarchical Transfer Learning for Multi-label Text Classification"
3. we randomly hold 5,000 examples from the original training set to be used as our development set."Generative and Discriminative Text Classification with Recurrent Neural Networks"
4. Twitter: This dataset is provided by NLTK4, a library of Python, which is also a binary sentiment classification dataset.
5. Add Word Avg Length of the Data set; Feature-based SVM
6. All datasets, train-test splits, and implementations of extreme classification algo- rithms were sourced from the Extreme Classification Repos- itory.
7. We employ Adam Optimizer [Kingma and Ba, 2014] to train our model. The initial learning rate is 0.01, the weight of L2 regularization term is 0.0001, the weight of context mo- ment loss is 1.5 and the dropout rate is 0.5. The dimension of LSTM hidden states and output representation is 50. The evaluation metrics are Accuracy and Macro-F1.
8. Specifically, we work with short posts from microblogs (Twitter) and social networks (Facebook), short text messages, extended discussion posts from 14 different Fortune 500 blogs, product reviews and their titles from an online shop (Amazon) as well as restaurant (Yelp) and movie reviews (IMDb, Rotten Tomatoes). 

### Dataset
1. 意图识别数据集：SWDA MRDA ATIS SNIPS：ProSeqo: Projection Sequence Networks for On-Device Text Classification
2. Twitter数据集F1 67.75 https://www.kaggle.com/c/ twitter-sentiment-analysis2
3. 文本分类数据集：1 http://nlp.stanford.edu/sentiment/  2 http://cogcomp.cs.illinois.edu/Data/QA/QC/ 3.http://www.di.unipi.it/ ̃gulli/AG_ corpus_of_news_articles.html
4. Twitter数据集：http://www.cs.cmu.edu/˜yww/data/petpeeves.zip

### Results
1. Table 3 shows the results of our model compared with the baselines. Our span-based method achieves significant im- provements over all the baselines in F1 score. 
2. To better understand how the components affect the com- plete TBSA task, we give a detailed analysis for the two sub- tasks.
3. Code Link:
4. Our models steadily outperforms the other two memory augmented networks. Particularly on Restaurant dataset, the improvement of ANTM+BERTL obtain more than 2% gain of Acc score compared with MemNet and RAM.
5. From the table, we can see that either using the first mo- ment only or using the second moment only can’t achieve good performance, for the reason that insufficient information is obtained.
6. This research attempts to fill this gap. 

### Conclusion and Future Work
1. And while performance gains seem clear for small datasets, EDA might not yield substantial improvements when using pre-trained models. "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"
2. Notably, much of the recent work in NLP focuses on making neural models larger or more complex. Our work, however, takes the opposite approach.  "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"
3. In this paper, we have proposed a simple yet effective recur- rent architecture named DenseRNNs, which borrows the idea of dense connections from DenseNets to the framework of RNNs. 
4. As future work, we plan to introduce additional decision variables to automatically tune the value of dense depth by the model itself. We also intend to apply the architecture to other RNN variants, and expect further performance gain with a better design of unit structure.
5. Our method is capa- ble of jointly leveraging the data correlations and multi-view complementary information, which is achieved by integrat- ing deep correlated predictive subspace learning and multi- view shared and private label prediction into a unified objec- tive function. Compared with the state-of-the-art multi-view semi-supervised learning methods, DCPSL can better han- dle the incomplete multi-view data and achieves competitive classification results on various practical datasets. "Deep Correlated Predictive Subspace Learning for Incomplete Multi-View Semi-Supervised Classification"
