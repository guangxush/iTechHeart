## I Love Papers

### Introduction
1. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds.
2. We find that the proposed model of tokenization provides an improvement in the performance of text classification with a simple LSTM classifier. 
3. This results in improved performance for sentiment analysis tasks on Japanese and English datasets and Chinese datasets with a larger cache. We find that the proposed model of tokenization provides an improvement in the performance of text classification with a simple LSTM classifier.
5. In recent years, lots of works have been done to solve text classification problems, but just a few of them have explored the explainability of their systems (Camburu et al., 2018; Ouyang et al., 2018). Ribeiro et al. (2016) try to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. 
6. To achieve these goals, in this paper, we propose a novel generative explanation framework for text classification, where our model is capable of not only providing the classification predictions but also generating fine-grained information as explanations for decisions. The novel idea behind our hybrid generative-discriminative method is to explicitly capture the fine-grained information inferred from raw texts, utilizing the information to help interpret the predicted classification results and improve the overall performance.
7. In this paper, we confirm that these models are useful for text classification when the number of labeled instances is small, but demonstrate that fine-tuning to in-domain data is also of critical importance. 
8. Inherent problems in data emerge in a trained model in several ways. Model explanations can show that the model is not inline with human judgment or domain expertise. A canonical example is model unfairness, which stems from biases in the training data. 
9. Our approach relies on re-using model parameters trained at upper levels in the taxonomy and fine-tuning them for classifying categories at lower levels.
10. One critical issue is that the number of local classifiers depends on the size of the label hierarchy, making local approaches infeasible to scale.
11. Most of the current effective methods for text classification task are based on large-scale labeled data and a great number of parameters, but when the supervised training data are few and difficult to be collected, these models are not available.
12. The dominant text classification models in deep learning require a considerable amount of labeled data to learn a large number of parameters. However, such methods may have difficulty in learning the semantic space in the case that only few data are available. 
13. However, it is not always realistic to assume that example labels are clean. Humans make mistakes and, depending on the complexity of the task, there may be disagreement even among expert labelers.
15. In the future, we plan to investigate broader applications like summarizaion, translation, question answering, etc.
16. As one of the most fundamental problems in machine learning, automatic classification has been widely studied in several domains. However, many approaches, proven to be effective in traditional classification tasks, cannot catch up with a dynamic and open environment where new classes can emerge after the learning stage (RomeraParedes and Torr, 2015). For example, the number of topics on social media is growing rapidly, and the classification models are required to recognise the text of the new topics using only general information (e.g., descriptions of the topics) since labelled training instances are unfeasible to obtain for each new topic (Lee et al., 2011). This scenario holds in many real-world domains such as object recognition and medical diagnosis (Xian et al., 2017; World Health Organization, 1996).
17. We also find that our architecture is insensitive to window size and enjoy a better robustness.
18. In many domains, however, labeled data is difficult to obtain. This might be due to annotation cost, or simply because there are no available instances to annotate before the system has to make the next classification decision.
19. Both methods promote rationale-like features in the input to improve the classification outcome. 
20. A recent strong performer in this line of research is BERT (Devlin et al., 2018), a multi-layer attention-based neural model that is pre-trained on large amounts of plain text and then fine tuned to specific tasks. Systems using BERT have achieved state-of-the-art performance on various NLP tasks, including short text classification.
21. They showed improved accuracy compared to non-rationale-augmented CNNs as well as to rationale-augmented SVM models. Our main contributions relative to that work are the introduction of the joint-learning technique, which learns text classification and rationale words identification concurrently, and the adaptation of the entire approach to BERT. The ability of our model to identify rationale spans within sentences also seems useful for interpretability.
22. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings.
23. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks.
25. This is significant improvement given ProSeqo’s small network size, and the fact that we used the same architecture and parameters across all seven NLP tasks, while prior work tuned depending on the task and dataset.
26. Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes.
27. We propose to induce the class-level representations from support sets to deal with samplewise diversity in few-shot learning tasks. 
28. We provide datasets for studying three aspects of 0SHOT-TC: topic categorization, emotion detection, and situation frame detection – an event level recognition problem. For each dataset, we have standard split for train, dev, and test, and standard separation of seen and unseen classes.
29. However, these methods cannot capture the semantic relations (e.g., entity relations) and rely heavily on the number of training data. Clearly, lacking of training data is still a key bottleneck that prohibits them from successful practical applications. ---"Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification"
30. With the rapid development of online social media and e-commerce, short texts, such as online news, queries, reviews, tweets, are increasingly widespread on the Internet (Song et al., 2014). Short text classification can be widely applied in many domains, ranging from sentiment analysis to news tagging/categorization and query intent classification.
31. n many practical scenarios, the labeled data is scarce, while human labeling is time-consuming and may require expert knowledge 
32. We propose a new recurrent convolutional model for text classification by discussing the shortcomings of max pooling operation and also the strength and weakness of convolution operation.
34. Since deep learning models always contain huge number of parameters, they require a large volume of labeled corpus to learn a good document representation. However, in many situations, it is difficult to construct large training sets because acquiring manually labeled documents is very expensive. In this case, the performance of such models would be limited due to the lack of training data.
35. onsist of product and movie reviews in 16 different domains. The data in each domain is randomly split into training set, development set and test set according to the proportion of 70%, 10%, 20%. Statistics of the 16 datasets are listed in Table 1. Multi-Domain Sentiment Classification Based on Domain-Aware Embedding and Attention.
36. Text classification is a fundamental text mining task including multi-class classification and multilabel classification. The former only assigns one label to the given document, while the latter classifies one document into different topics. In this paper, we focus on multi-label text classification (MLTC) because it has become one of the core tasks in natural language processing and has been widely applied in topic recognition.
37. CNN: However, CNNs which were first proposed for images, face many crucial challenges in the context of text processing, namely in their elementary blocks: convolution filters and max pooling. These challenges have largely been overlooked by the most existing CNN models proposed for text classification. "Sequential Learning of Convolutional Features for Effective Text Classification"
38. Frequency based feature selection techniques like Bag-ofWords (BoW), Term frequency-Inverse document frequency (TF-IDF) have been used and the features are trained using machine learning classifiers like Logistic Regression (LR) or Naive Bayes (NB). These approaches provided strong baselines for text classification.
39. However, sequential patterns and semantic structure between words play a crucial role in deciding the category of a text. Traditional lexicon approaches fail to capture such complexities. Since the success of deep learning, neural network architectures have outperformed traditional methods like BoW and count based feature selection techniques.
40. Traditional text classifiers are limited to predicting over a fixed set of labels. However, in many real-world applications the label set is frequently changing. For example, in intent classification, new intents may be added over time while others are removed. "Metric Learning for Dynamic Text Classification"
41. We propose to address the problem of dynamic text classification by replacing the traditional, fixed-size output layer with a learned, semantically meaningful metric space. "Metric Learning for Dynamic Text Classification"
42. In recent years, many successful deep learning models have been widely applied to this task. Mainstream deep learning methods usually contain two important components: representation learning and feature aggregation. 
43. Full training of a new model for each new task is prohibitive in terms of memory, due to the shear number of parameter of deep networks. Instead, new tasks should be learned incrementally, building on prior knowledge from already learned tasks, and without catastrophic forgetting, i.e. without hurting performance on prior tasks. 
44. Currently, the structure features in most structure representation models are either provided as input from pre-defined parsers or learned end-to-end from feature detectors, such as CNN and RNN. However, these representation methods are not automatical and unable to learn adaptive text structure sufficiently. Focusing on CNN encoder, the traditional pipeline is using convolutional layers with geometrically fixed filters to extract spatial features (n-gram structure) in the last level of output, whatever for shallow or deep encoders. Then a following pooling or attention layer is utilized to aggregate prominent features for downstream tasks. The main limitations include that little structure information is explicitly learned from the encoder, and the aggregation strategies perform selection directly over structure representations, which can’t fully utilize their latent semantic abstractions. "Dynamically Route Hierarchical Structure Representation to Attentive Capsule for Text Classification"
45. Pre-trained word embeddings likes ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance in various tasks.
46. BERT [Devlin et al., 2018] is a pre-trained language model with deep bidirectional transformer. Finetuning BERT model can outperform state-of-the-art models among many tasks, including text classification. We use the pre-trained BERT-base model available online and finetune it on each task.

### Related Work
1. One of the main reasons our approach improves the model in the original task is that the model is now more robust thanks to the reinforcement provided to the model builder through attributions. From a fairness angle, our technique shares similarities with adversarial training (Zhang et al., 2018a; Madras et al., 2018) in asking the model to optimize for an additional objective that transitively unbiases the classifier. "Incorporating Priors with Feature Attribution on Text Classification"
2. NeuralClassifier reimplements a very large number of the state-of-the-art text encoders, including FastText (Joulin et al., 2016), TextCNN (Kim, 2014), TextRNN (Liu et al., 2016), RCNN (Lai et al., 2015) , VDCNN (Conneau et al., 2016), DPCNN (Johnson and Zhang, 2017) , AttentiveConvNet (Yin and Schu ̈tze, 2017), DRNN (Wang, 2018), Transformer encoder (Vaswani et al., 2017), Star-Transformer encoder (Guo et al., 2019).
3. TF-IDF (task-agnostic) This algorithm views the vocabulary selection as a retrieval problem (Ramos et al., 2003), where term frequency is viewed as the word frequency and document frequency is viewed as the number of sentences where such word appears. 
4. However, as the order in similarity between SVM and XGBoost is less stable, entropy cannot be the sole cause. 
5. Our results show that different approaches can sometimes lead to very different important features, but there exist some consistent patterns between models and methods. For instance, deep learning models tend to generate diverse important features that are different from traditional models; post-hoc methods lead to more similar important features than built-in methods.

### Experimental Configurations
1. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task.
2. We did study more complicated relationships between ai and bj with multilayer perceptrons, but observed no further improvement on the heldout data.
3. We tried to apply the F(:) function on our hidden states before computing eij and it did not further help our models.
4. We also further modeled the interaction by feeding the tuples into feedforward neural networks and addeds the top layer hidden states to the above concatenation.
5. We found that it does not further help the inference accuracy on the held out dataset.
6. We apply a grid search for hyper-parameters: the learning rate is tuned amongst {0.05, 0.01, 0.005, 0.001}, the coefficient of L2 normalization is searched in {10−5, 10−4, · · · , 101, 102}, and the dropout ratio is tuned in {0.0, 0.1, · · · , 0.8} for NFM, GC-MC, and KGAT. Besides, we employ the node dropout technique for GC-MC and KGAT, where the ratio is searched in {0.0, 0.1, · · · , 0.8}. We select the best parameters depending on the performance of validation data.    
7. Each experiment is repeated 3 times, and the average performance is reported.
8. The settings of hyper-parameters are determined by optimizing AUC on a validation set.
9. We randomly split the dataset into 10919/1373/1356 pairs for train/dev/test set. The distribution of the overall rating scores within this corpus is shown in Table 1. 
10. we use 300-dimensional word2vec [Mikolov et al., 2013] vectors to initialize word representations. 
11. 70% of the documents of each class were randomly selected for training, and the remaining 30% were used as a testing set. "Integrating Semantic Knowledge to Tackle Zero-shot Text Classification"
12. These configurations are set on the validation set held out by 10% from the training data. If not specified, the same configurations are used in all the datasets. "Adaptive Convolution for Text Classification"
13. No matter which method we use, important features from SVM and XGBoost are more similar with each other, than with deep learning models (Figure 2). "Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification"
14. For all the tasks, we use 20% of the dataset as the test set. For SVM and XGBoost, we use cross validation on the other 80% to tune hyperparameters. For LSTM with attention and BERT, we use 10% of the dataset as a validation set, and choose the best hyperparameters based on the validation performance. "Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification"
15. For each dataset, we randomly split the full training corpus into training and validation set, where the validation size is the same as the corresponding test size. Then the validation set is fixed for all models for fair comparison. 
16. The convolution layer had three filter sizes [2, 3, 4] with 50 filters for each size, while the intermediate fully-connected layer had 150 units. The activation functions of the filters and the fully-connected layers are ReLU (except the softmax at the output layer). The models were implemented using Keras and trained with Adam optimizer. The macro-average F1 are 0.90 and 0.94 for the Amazon and the ArXiv datasets, respectively. Overall, the ArXiv appears to be an easier task as it is likely solvable by looking at individual keywords. "Human-grounded Evaluations of Explanation Methods for Text Classification"
17. SVM: SVM classifiers using TF-IDF features and LDA features (Blei et al., 2003), are denoted as SVM+TFIDF and SVM+LDA, respectively. "Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification "
18. CNN: CNN (Kim, 2014) with 2 variants: 1) CNN-rand, whose word embeddings are randomly initialized, and 2) CNN-pretrain, whose word embeddings are pre-trained with Wikipedia Corpus. "Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification "
19. LSTM: LSTM (Liu et al., 2016) with and without pre-trained word embeddings, named LSTMrand and LSTM-pretrain, respectively. "Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification"
20. RF is an ensemble learning method that grows a multitude of randomized, uncorrelated decision trees (Breiman, 2001). 
21. SVM are discriminative classifiers, fitting a margin-maximizing hyperplane between classes. They were initially developed as binary linear classifiers (Cortes & Vapnik, 1995), but can be extended to non-linear problems of higher dimensionality through the use of kernels that can accommodate any functional form (Scholkopf & Smola, 2001). 
22. We also demonstrate that using BERT representations further substantially boosts the performance.


### Dataset and Hyperparameters Setting
1. Number of samples are 95,692 / 32,128 / 31,866 in the train/dev/test sets respectively. "Incorporating Priors with Feature Attribution on Text Classification"
2. The data corresponding to each category was randomly split into 85% training and 15% validation instances. We restrict the documents in the dataset to a maximum of 100 words. "Hierarchical Transfer Learning for Multi-label Text Classification"
3. we randomly hold 5,000 examples from the original training set to be used as our development set."Generative and Discriminative Text Classification with Recurrent Neural Networks"
4. Twitter: This dataset is provided by NLTK4, a library of Python, which is also a binary sentiment classification dataset.
5. Add Word Avg Length of the Data set; Feature-based SVM
6. All datasets, train-test splits, and implementations of extreme classification algorithms were sourced from the Extreme Classification Repository.
7. We employ Adam Optimizer [Kingma and Ba, 2014] to train our model. The initial learning rate is 0.01, the weight of L2 regularization term is 0.0001, the weight of context moment loss is 1.5 and the dropout rate is 0.5. The dimension of LSTM hidden states and output representation is 50. The evaluation metrics are Accuracy and Macro-F1.
8. Specifically, we work with short posts from microblogs (Twitter) and social networks (Facebook), short text messages, extended discussion posts from 14 different Fortune 500 blogs, product reviews and their titles from an online shop (Amazon) as well as restaurant (Yelp) and movie reviews (IMDb, Rotten Tomatoes). 
9. The parameters of all the compared methods are set as suggested in the corresponding papers. The parameters of our method are determined by five fold cross-validation. λ and α are selected from {10−3, 10−2,..., 102}. β1 and β2 are selected from {0.005, 0.01,..., 50}. All the experiments are repeated ten times and the averaged performance are reported. For the evaluation metric, we follow [Feiping Nie and Li, 2018] and use accuracy for performance evaluation, which calculates the proportion of the correctly classified samples.
10. we conduct classification experiments by changing the incomplete rate ε% from {0, 10%, 30%, 50%, 70%} while fixing the percentage of labeled samples to 0.3. 
11. We train our model’s parameters using gradient-based optimizer Adam, with an initial learning rate 1e-3.
12. The training objective is to minimize the cross-entropy loss.
13. We hand-tuned the learning rate of the incremental runs for we want them to train for certain epochs. We observe that we can achieve better performance if we incrementally train from the existing 195-animal-model (36.8% versus 38.6%), taking slightly less time than the scratch run. On the other hand, with 25% few data (stopping with 30 epochs), incremental learning already beating the from-scratch one (37.9% versus 38.6%). "Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classification"
14. In our experiments, we used 70 percent of each labeled dataset and 100 percent of each unlabeled dataset to train the prediction models; for testing purposes, we used the remaining 30 percent of each labeled dataset. The value of m is set to 50,000 and that of q to 5,000 in the experiments. In scenarios in which plentiful computational resources are available, we can set m to a low value such as 1,000 (with the effect that more frequent updates are made in the ensemble, which, in turn, increase the system’s holistic predictive performance, especially in streaming environments) and q to a high value such as 50,000, which would also lead to a better update (and hence better accuracy) of the ensemble. "Online URL Classification for Large-Scale Streaming Environments "
15. To obtain an unbiased estimate of out-of-sample accuracy, we split each dataset into a training set (80% of the data) and a hold-out test set (20% of the data). All accuracies reported in this paper are based on predictions on the hold-out test set (see Ordenes et al., 2018 for a similar approach). 
16. We also demonstrate that using BERT representations further substantially boosts the performance.
17. When we switch from GloVe embeddings to BERT representations, the training time for a three-layer TD-GAT model on the restaurant dataset only increases from 1.12 seconds/epoch to 1.15 seconds/epoch. On the con- trary, fine-tuning the BERT model takes about 226.50 seconds for each training epoch. Training our TD-GAT-BERT model requires much fewer computation resources and less time compared to fine-tuning the original BERT model.
18. Using GloVe embeddings, our approach TD-GAT-GloVe outperforms various baseline models. After switching to BERT repre- sentations, we show that TD-GAT-BERT achieves much better performance. It is lightweight and requires fewer computational resources and less training time than fine-tuning the original BERT model.
19. For training, we use a mini-batch size of 64 and documents of similar length (in terms of the number of sentences in the documents) are organized to be a batch. We find that length-adjustment can accelerate training by three times. We use stochastic gradient descent to train all models with momentum of 0.9. We pick the best learning rate using grid search on the validation set.
20. The hyper parameters of the models are tuned on the validation set. In our experiments, we set the word embedding dimension to be 200 and the GRU dimension to be 50.
21. F1-score is used as evaluation metric for SF task. A slot is considered to be correct if its range and type are correct. The  F1-score is calculated using CoNLL evaluation scrip

### Dataset
1. 意图识别数据集：SWDA MRDA ATIS SNIPS：ProSeqo: Projection Sequence Networks for On-Device Text Classification
2. Twitter数据集F1 67.75 https://www.kaggle.com/c/ twitter-sentiment-analysis2
3. 文本分类数据集：1 http://nlp.stanford.edu/sentiment/  2 http://cogcomp.cs.illinois.edu/Data/QA/QC/ 3.http://www.di.unipi.it/ ̃gulli/AG_ corpus_of_news_articles.html
4. Twitter数据集：http://www.cs.cmu.edu/˜yww/data/petpeeves.zip
5. 对话意图数据集：CSID Character Studio Intention Detection is a dataset extracted from a real-world open domain chatbot. 


### Results
1. Table 3 shows the results of our model compared with the baselines. Our span-based method achieves significant improvements over all the baselines in F1 score. 
2. To better understand how the components affect the complete TBSA task, we give a detailed analysis for the two subtasks.
3. Code Link:
4. Our models steadily outperforms the other two memory augmented networks. Particularly on Restaurant dataset, the improvement of ANTM+BERTL obtain more than 2% gain of Acc score compared with MemNet and RAM.
5. From the table, we can see that either using the first moment only or using the second moment only can’t achieve good performance, for the reason that insufficient information is obtained.
6. This research attempts to fill this gap. 

### Conclusion and Future Work
1. And while performance gains seem clear for small datasets, EDA might not yield substantial improvements when using pre-trained models. "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"
2. Notably, much of the recent work in NLP focuses on making neural models larger or more complex. Our work, however, takes the opposite approach.  "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"
3. In this paper, we have proposed a simple yet effective recurrent architecture named DenseRNNs, which borrows the idea of dense connections from DenseNets to the framework of RNNs. 
4. As future work, we plan to introduce additional decision variables to automatically tune the value of dense depth by the model itself. We also intend to apply the architecture to other RNN variants, and expect further performance gain with a better design of unit structure.
5. Our method is capable of jointly leveraging the data correlations and multi-view complementary information, which is achieved by integrating deep correlated predictive subspace learning and multiview shared and private label prediction into a unified objective function. Compared with the state-of-the-art multi-view semi-supervised learning methods, DCPSL can better handle the incomplete multi-view data and achieves competitive classification results on various practical datasets. "Deep Correlated Predictive Subspace Learning for Incomplete Multi-View Semi-Supervised Classification"
6. We adopt the widely used Q-learning algorithm as the basic implementation for IRL, which can be extended to other specific RL algorithms (e.g., SARSA [43]) in principle. Our future work will focus on the statistical analysis of IRL in more complex dynamic environments and automatical optimal selection of the neighborhood degree m.
7. For the future work, a couple of other classification methods including logistic regression, support vector machine, decision tree, and random forests will be used to compare with the suggested deep learning neural network. Grid search strategy will also be used to tune hyper-parameters in the learning algorithms, in order to further improve the classification performance of the algorithms. Further, other performance metrics such as AUC, precision, recall, and F1 score can be applied to more comprehensively evaluate the performance of deep learning classification method. "A Classification Framework for Online Social Support Using Deep Learning"
