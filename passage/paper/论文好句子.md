### Dataset
1. 意图识别数据集：SWDA MRDA ATIS SNIPS：ProSeqo: Projection Sequence Networks for On-Device Text Classification
2. Twitter数据集F1 67.75 https://www.kaggle.com/c/ twitter-sentiment-analysis2
3. 文本分类数据集：1 http://nlp.stanford.edu/sentiment/  2 http://cogcomp.cs.illinois.edu/Data/QA/QC/ 3.http://www.di.unipi.it/ ̃gulli/AG_ corpus_of_news_articles.html
4. Twitter数据集：http://www.cs.cmu.edu/˜yww/data/petpeeves.zip

### Introduction
1. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds.
2. We find that the proposed model of tokenization provides an improvement in the performance of text classification with a simple LSTM classifier. 
3. This results in improved performance for sentiment analysis tasks on Japanese and English datasets and Chinese datasets with a larger cache. We find that the proposed model of tokenization provides an improvement in the performance of text classification with a simple LSTM classifier.
5. In recent years, lots of works have been done to solve text classification problems, but just a few of them have explored the explainability of their systems (Camburu et al., 2018; Ouyang et al., 2018). Ribeiro et al. (2016) try to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. 
6. To achieve these goals, in this paper, we propose a novel generative explanation framework for text classification, where our model is capable of not only providing the classification predictions but also generating fine-grained information as explanations for decisions. The novel idea behind our hybrid generative-discriminative method is to explicitly capture the fine-grained information inferred from raw texts, utilizing the information to help interpret the predicted classification results and improve the overall performance.
7. In this paper, we confirm that these models are useful for text classification when the number of labeled instances is small, but demonstrate that fine-tuning to in-domain data is also of critical importance. 
8. Inherent problems in data emerge in a trained model in several ways. Model explanations can show that the model is not inline with human judgment or domain expertise. A canonical example is model unfairness, which stems from biases in the training data. 
9. Our approach relies on re-using model parameters trained at upper levels in the taxonomy and fine-tuning them for classifying categories at lower levels.
10. One critical issue is that the number of local classifiers depends on the size of the label hierarchy, making local approaches infeasible to scale.
11. Most of the current effective methods for text classification task are based on large-scale labeled data and a great number of parameters, but when the supervised training data are few and difficult to be collected, these models are not available.
12. The dominant text classification models in deep learning require a considerable amount of labeled data to learn a large number of parameters. However, such methods may have difficulty in learning the semantic space in the case that only few data are available. 
13. However, it is not always realistic to assume that example labels are clean. Humans make mistakes and, depending on the complexity of the task, there may be disagreement even among expert labelers.
15. In the future, we plan to investigate broader applications like summarizaion, translation, question answering, etc.
16. As one of the most fundamental problems in machine learning, automatic classification has been widely studied in several domains. However, many approaches, proven to be effective in traditional classification tasks, cannot catch up with a dynamic and open environment where new classes can emerge after the learning stage (RomeraParedes and Torr, 2015). For example, the number of topics on social media is growing rapidly, and the classification models are required to recognise the text of the new topics using only general information (e.g., descriptions of the topics) since labelled training instances are unfeasible to obtain for each new topic (Lee et al., 2011). This scenario holds in many real-world domains such as object recognition and medical diagnosis (Xian et al., 2017; World Health Organization, 1996).
17. We also find that our architecture is insensitive to window size and enjoy a better robustness.
18. In many domains, however, labeled data is difficult to obtain. This might be due to annotation cost, or simply because there are no available instances to annotate before the system has to make the next classification decision.
19. Both methods promote rationale-like features in the input to improve the classification outcome. 
20. A recent strong performer in this line of research is BERT (Devlin et al., 2018), a multi-layer attention-based neural model that is pre-trained on large amounts of plain text and then fine tuned to specific tasks. Systems using BERT have achieved state-of-the-art performance on various NLP tasks, including short text classification.
21. They showed improved accuracy compared to non-rationale-augmented CNNs as well as to rationale-augmented SVM models. Our main contributions relative to that work are the introduction of the joint-learning technique, which learns text classification and rationale words identification concurrently, and the adaptation of the entire approach to BERT. The ability of our model to identify rationale spans within sentences also seems useful for interpretability.
22. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings.
23. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks.
25. This is significant improvement given ProSeqo’s small network size, and the fact that we used the same architecture and parameters across all seven NLP tasks, while prior work tuned depending on the task and dataset.
26. Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes.
27. We propose to induce the class-level representations from support sets to deal with samplewise diversity in few-shot learning tasks. 
28. We provide datasets for studying three aspects of 0SHOT-TC: topic categorization, emotion detection, and situation frame detection – an event level recognition problem. For each dataset, we have standard split for train, dev, and test, and standard separation of seen and unseen classes.
29. However, these methods cannot capture the semantic relations (e.g., entity relations) and rely heavily on the number of training data. Clearly, lacking of training data is still a key bottleneck that prohibits them from successful practical applications. ---"Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification"
30. With the rapid development of online social media and e-commerce, short texts, such as online news, queries, reviews, tweets, are increasingly widespread on the Internet (Song et al., 2014). Short text classification can be widely applied in many domains, ranging from sentiment analysis to news tagging/categorization and query intent classification.
31. n many practical scenarios, the labeled data is scarce, while human labeling is time-consuming and may require expert knowledge 
32. We propose a new recurrent convolutional model for text classification by discussing the shortcomings of max pooling operation and also the strength and weakness of convolution operation.
34. Since deep learning models always contain huge number of parameters, they require a large volume of labeled corpus to learn a good document representation. However, in many situations, it is difficult to construct large training sets because acquiring manually labeled documents is very expensive. In this case, the performance of such models would be limited due to the lack of training data.
35. onsist of product and movie reviews in 16 different domains. The data in each domain is randomly split into training set, development set and test set according to the proportion of 70%, 10%, 20%. Statistics of the 16 datasets are listed in Table 1. Multi-Domain Sentiment Classification Based on Domain-Aware Embedding and Attention.

### Experimental Configurations
1. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task.
2. We did study more complicated relationships between ai and bj with multilayer perceptrons, but observed no further improvement on the heldout data.
3. We tried to apply the F(:) function on our hidden states before computing eij and it did not further help our models.
4. We also further modeled the interaction by feeding the tuples into feedforward neural networks and addeds the top layer hidden states to the above concatenation.
5. We found that it does not further help the inference accuracy on the held out dataset.
6. We apply a grid search for hyper-parameters: the learning rate is tuned amongst {0.05, 0.01, 0.005, 0.001}, the coefficient of L2 normalization is searched in {10−5, 10−4, · · · , 101, 102}, and the dropout ratio is tuned in {0.0, 0.1, · · · , 0.8} for NFM, GC-MC, and KGAT. Besides, we employ the node dropout technique for GC-MC and KGAT, where the ratio is searched in {0.0, 0.1, · · · , 0.8}. We select the best parameters depending on the performance of validation data.    
7. Each experiment is repeated 3 times, and the average performance is reported.
8. The settings of hyper-parameters are determined by optimizing AUC on a validation set.


    
    