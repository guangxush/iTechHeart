### Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings

In this work, we explore an unsupervised approach to classify documents into categories simply described by a label. The proposed method is inspired by the way a human proceeds in this situation: It draws on textual similarity between the most relevant words in each document and a dic- tionary of keywords for each category reflect- ing its semantics and lexical field. The nov- elty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific. Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches. It thus provides a practical alternative to situations where low- cost text categorization is needed, as we illus- trate with our application to operational risk incidents classification.

In this paper, we present a method for unsuper- vised text classification based on computing the similarity between the documents to be classi- fied and a rich description of the categories label. The category label enrichment starts with human- expert provided keywords but is then expanded through the use of word embeddings. We also investigated whether a consolidation step that re- moves non discriminant words from the label dic- tionaries could have an effect on performance.

We have not explored whether recent advances in word embeddings from instance ELMO (Peters et al., 2018) and BERT (Devlin et al., 2018) could add further benefits. This is certainly an avenue that we seek to explore. However, for our applica- tion domain, we expect that it may not lead to in- creased performance as words are used to a large extent with the same sense across the corpus.

相似度计算，无监督学习，文本分类

### Stochastic Tokenization with a Language Model for Neural Text Classification

Sentences are usually seg- mented with words or subwords by a morpho- logical analyzer or byte pair encoding and then encoded with word (or subword) representa- tions for neural networks. However, segmenta- tion is potentially ambiguous, and it is unclear whether the segmented tokens achieve the best performance for the target task. In this pa- per, we propose a method to simultaneously learn tokenization and text classification to ad- dress these problems. Our model incorporates a language model for unsupervised tokeniza- tion into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We con- ducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than pre- vious methods.

Twitter数据集F1 67.75 https://www.kaggle.com/c/ twitter-sentiment-analysis2

### Towards Explainable NLP: A Generative Explanation Framework for Text Classification
Building explainable systems is a critical prob- lem in the field of Natural Language Process- ing (NLP), since most machine learning mod- els provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpret- ing the outputs or the connections between in- puts and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable ex- planations. To solve this problem, we pro- pose a novel generative explanation frame- work that learns to make classification deci- sions and generate fine-grained explanations at the same time. More specifically, we intro- duce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rat- ing scores, and fine-grained reasons. We con- duct experiments on both datasets, compar- ing with several strong neural network base- line systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise expla- nations at the same time.

Since our proposed framework is model- agnostic, we can combine it with other natural processing tasks, e.g. summarization, extraction, which we leave to our future work.
可解释的机器学习模型

### Variational Pretraining for Semi-supervised Text Classification
We introduce VAMPIRE,1 a lightweight pre- training framework for effective text classi- fication when data and computing resources are limited. We pretrain a unigram docu- ment model as a variational autoencoder on in-domain, unlabeled data and use its inter- nal states as features in a downstream classi- fier. Empirically, we show the relative strength of VAMPIRE against computationally expen- sive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in- domain data is crucial to achieving decent per- formance from contextual embeddings when working with limited supervision. We accom- pany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks.
轻量级的文本分类框架

### Incorporating Priors with Feature Attribution on Text Classification
Feature attribution methods, proposed re- cently, help users interpret the predictions of complex models. Our approach integrates fea- ture attributions into the objective function to allow machine learning practitioners to incor- porate priors in model building. To demon- strate the effectiveness our technique, we ap- ply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in a scarce data setting by forcing the model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the objective. Our experiments show that i) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating priors helps model perfor- mance in scarce data settings.

Hence, most machine learning progress made in those areas is hindered by a lack of model explain- ability – causing practitioners to resort to simpler, potentially low-performance models.

### Hierarchical Transfer Learning for Multi-label Text Classification

Multi-Label Hierarchical Text Classification (MLHTC) is the task of categorizing docu- ments into one or more topics organized in an hierarchical taxonomy. MLHTC can be for- mulated by combining multiple binary classifi- cation problems with an independent classifier for each category. We propose a novel trans- fer learning based strategy, HTrans, where bi- nary classifiers at lower levels in the hier- archy are initialized using parameters of the parent classifier and fine-tuned on the child category classification task. In HTrans, we use a Gated Recurrent Unit (GRU)-based deep learning architecture coupled with attention. Compared to binary classifiers trained from scratch, our HTrans approach results in signifi- cant improvements of 1% on micro-F1 and 3% on macro-F1 on the RCV1 dataset. Our exper- iments also show that binary classifiers trained from scratch are significantly better than single multi-label models.

### Large-Scale Multi-Label Text Classification on EU Legislation
We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with ∼4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Exper- imenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings fur- ther improve performance. We also find that considering only particular zones of the docu- ments is sufficient. This allows us to bypass BERT’s maximum text length limit and fine- tune BERT, obtaining the best results in all but zero-shot learning cases.


### NeuralClassifier: An Open-source Neural Hierarchical Multi-label Text Classification Toolkit
In this paper, we introduce NeuralClassifier, a toolkit for neural hierarchical multi-label text classification. NeuralClassifier is designed for quick implementation of neural models for hierarchical multi-label classification task, which is more challenging and common in real-world scenarios. A salient feature is that NeuralClassifier currently provides a variety of text encoders, such as FastText, TextCNN, TextRNN, RCNN, VDCNN, DPCNN, DRNN, AttentiveConvNet and Transformer encoder, etc. It also supports other text classification scenarios, including binary-class and multi- class classification. Built on PyTorch1 , the core operations are calculated in batch, mak- ing the toolkit efficient with the acceleration of GPU. Experiments show that models built in our toolkit achieve comparable performance with reported results in the literature

### Hierarchical Text Classification with Reinforced Label Assignment
While existing hierarchical text classification (HTC) methods attempt to capture label hier- archies for model training, they either make local decisions regarding each label or com- pletely ignore the hierarchy information dur- ing inference. To solve the mismatch between training and inference as well as modeling la- bel dependencies in a more principled way, we formulate HTC as a Markov decision pro- cess and propose to learn a Label Assignment Policy via deep reinforcement learning to de- termine where to place an object and when to stop the assignment process. The proposed method, HiLAP, explores the hierarchy dur- ing both training and inference time in a con- sistent manner and makes inter-dependent de- cisions. As a general framework, HiLAP can incorporate different neural encoders as base models for end-to-end training. Experiments on five public datasets and four base models show that HiLAP yields an average improve- ment of 33.4% in Macro-F1 over flat classifiers and outperforms state-of-the-art HTC methods by a large margin.
We proposed an end-to-end reinforcement learn- ing approach to hierarchical text classification (HTC) where objects are labeled by placing them at the proper positions in the label hierarchy. The proposed framework makes consistent and inter-dependent predictions, in which any neural- based representation learning model can be used as a base model and a label assignment policy is learned to determine where to place the objects and when to stop the assignment process. Exper- iments on five public datasets and four base mod- els showed that our approach outperforms state- of-the-art HTC methods significantly. 

### Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification
As an essential component of natural language processing, text classification relies on deep learning in recent years. Various neural net- works are designed for text classification on the basis of word embedding. However, pol- ysemy is a fundamental feature of the natu- ral language, which brings challenges to text classification. One polysemic word contains more than one sense, while the word embed- ding procedure conflates different senses of a polysemic word into a single vector. Extract- ing the distinct representation for the specific sense could thus lead to fine-grained models with strong generalization ability. It has been demonstrated that multiple senses of a word actually reside in linear superposition within the word embedding so that specific senses can be extracted from the original word embed- ding. Therefore, we propose to use capsule networks to construct the vectorized represen- tation of semantics and utilize hyperplanes to decompose each capsule to acquire the spe- cific senses. A novel dynamic routing mech- anism named ‘routing-on-hyperplane’ will se- lect the proper sense for the downstream clas- sification task. Our model is evaluated on 6 different datasets, and the experimental re- sults show that our model is capable of ex- tracting more discriminative semantic features and yields a significant performance gain com- pared to other baseline methods.


### Label-Specific Document Representation for Multi-Label Text Classification
Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper, we propose a Label-Specific Attention Network (LSAN) to learn the new document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile, the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to seamlessly integrate the above two parts, an adaptive fusion strategy is designed, which can effectively output the comprehensive document representation to build multilabel text classifier. Extensive experimental results on four benchmark datasets demonstrate that LSAN consistently outperforms the stateof-the-art methods, especially on the prediction of low-frequency labels.

A new label-specific attention network, in this pa- per, is proposed for multi-label text classification. It makes use of document content and label text to learn the label-specific document representation with the aid of self-attention and label-attention mechanisms. An adaptive fusion is designed to ef- fectively integrate these two attention mechanisms to improve the final prediction performance. Ex- tensive experiments on four benchmark dataset- s prove the superiority of LSAN by comparing with the state-of-the-art methods, especially on the dataset with large subset of low-frequency labels.

### Hierarchical Attention Prototypical Networks for Few-Shot Text Classification
Most of the current effective methods for text classification task are based on large-scale la- beled data and a great number of parame- ters, but when the supervised training data are few and difficult to be collected, these mod- els are not available. In this paper, we pro- pose a hierarchical attention prototypical net- works (HAPN) for few-shot text classifica- tion. We design the feature level, word level, and instance level multi cross attention for our model to enhance the expressive ability of se- mantic space. We verify the effectiveness of our model on two standard benchmark few- shot text classification datasets - FewRel and CSID, and achieve the state-of-the-art perfor- mance. The visualization of hierarchical atten- tion layers illustrates that our model can cap- ture more important features, words, and in- stances separately. In addition, our attention mechanism increases support set augmentabil- ity and accelerates convergence speed in the training stage.

### An Effective Label Noise Model for DNN Text Classification
Because large, human-annotated datasets suf- fer from labeling errors, it is crucial to be able to train deep neural networks in the pres- ence of label noise. While training image classification models with label noise have re- ceived much attention, training text classifica- tion models have not. In this paper, we pro- pose an approach to training deep networks that is robust to label noise. This approach in- troduces a non-linear processing layer (noise model) that models the statistics of the la- bel noise into a convolutional neural network (CNN) architecture. The noise model and the CNN weights are learned jointly from noisy training data, which prevents the model from overfitting to erroneous labels. Through ex- tensive experiments on several text classifica- tion datasets, we show that this approach en- ables the CNN to learn better sentence repre- sentations and is robust even to extreme label noise. We find that proper initialization and regularization of this noise model is critical. Further, by contrast to results focusing on large batch sizes for mitigating label noise for image classification, we find that altering the batch size does not have much effect on classifica- tion performance.

给出了一些数据集可以参考SST-2, TREC, AGNEWS, DBPedia


### How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection

With the rapid development in deep learn- ing, deep neural networks have been widely adopted in many real-life natural language ap- plications. Under deep neural networks, a pre- defined vocabulary is required to vectorize text inputs. The canonical approach to select pre- defined vocabulary is based on the word fre- quency, where a threshold is selected to cut off the long tail distribution. However, we observed that such a simple approach could easily lead to under-sized vocabulary or over- sized vocabulary issues. Therefore, we are interested in understanding how the end-task classification accuracy is related to the vocab- ulary size and what is the minimum required vocabulary size to achieve a specific perfor- mance. In this paper, we provide a more sophisticated variational vocabulary dropout (VVD) based on variational dropout to per- form vocabulary selection, which can intelli- gently select the subset of the vocabulary to achieve the required performance. To eval- uate different algorithms on the newly pro- posed vocabulary selection problem, we pro- pose two new metrics: Area Under Accuracy- Vocab Curve and Vocab Size under X% Ac- curacy Drop. Through extensive experiments on various NLP classification tasks, our varia- tional framework is shown to significantly out- perform the frequency-based and other selec- tion baselines on these metrics.

In this paper, we propose a vocabulary selection algorithm which can find sparsity in the vocabu- lary and dynamically decrease its size to contain only the useful words. Through our experiments, we have empirically demonstrated that the com- monly adopted frequency-based vocabulary selec- tion is already a very strong mechanism, further applying our proposed VVD can further improve the compression ratio. However, due to the time and memory complexity issues, our algorithm and evaluation are more suitable for classification- based application. 

### Integrating Semantic Knowledge to Tackle Zero-shot Text Classification

Insufficient or even unavailable training data of emerging classes is a big challenge of many classification tasks, including text classification. Recognising text documents of classes that have never been seen in the learning stage, so-called zero-shot text classification, is there- fore difficult and only limited previous works tackled this problem. In this paper, we pro- pose a two-phase framework together with data augmentation and feature augmentation to solve this problem. Four kinds of semantic knowledge (word embeddings, class descrip- tions, class hierarchy, and a general knowl- edge graph) are incorporated into the pro- posed framework to deal with instances of un- seen classes effectively. Experimental results show that each and the combination of the two phases achieve the best overall accuracy com- pared with baselines and recent approaches in classifying real-world texts under the zero-shot scenario.


### Adaptive Convolution for Text Classification

In this paper, we present an adaptive convolution for text classification to give stronger flexibility to convolutional neural networks (CNNs). Unlike traditional convolutions that use the same set of filters regardless of different inputs, the adaptive convolution employs adaptively generated convolutional filters that are conditioned on inputs. We achieve this by attaching filter-generating networks, which are carefully designed to generate input-specific filters, to convolution blocks in existing CNNs. We show the efficacy of our approach in existing CNNs based on our performance evaluation. Our evaluation indicates that adaptive convolutions improve all the baselines, without any exception, as much as up to 2.6 percentage point in seven benchmark text classification datasets.

In this paper, we have introduced the adaptive convolution to endow flexibility to convolution operations. Further, we have proposed the hashing technique which can drastically reduce the number of parameters for adaptive convolutions. We have validated our approach based on the performance evaluation with seven datasets, and investigated the effectiveness of adaptive convolutions through analysis. We believe that our methodology is applicable to other NLP tasks with text pairs, such as textual entailment, question answering. We plan to apply the proposed approach to those tasks in the future.

### Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification

Feature importance is commonly used to ex- plain machine predictions. While feature im- portance can be derived from a machine learn- ing model with a variety of methods, the con- sistency of feature importance via different methods remains understudied. In this work, we systematically compare feature importance from built-in mechanisms in a model such as attention values and post-hoc methods that ap- proximate model behavior such as LIME. Us- ing text classification as a testbed, we find that 1) no matter which method we use, impor- tant features from traditional models such as SVM and XGBoost are more similar with each other, than with deep learning models; 2) post- hoc methods tend to generate more similar im- portant features for two models than built-in methods. We further demonstrate how such similarity varies across instances. Notably, im- portant features do not always resemble each other better when two models agree on the pre- dicted label than when they disagree.


### Enhancing Local Feature Extraction with Global Representation for Neural Text Classification

For text classification, traditional local fea- ture driven models learn long dependency by deeply stacking or hybrid modeling. This pa- per proposes a novel Encoder1-Encoder2 ar- chitecture, where global information is incor- porated into the procedure of local feature extraction from scratch. In particular, En- coder1 serves as a global information provider, while Encoder2 performs as a local feature extractor and is directly fed into the classi- fier. Meanwhile, two modes are also designed for their interactions. Thanks to the aware- ness of global information, our method is able to learn better instance specific local features and thus avoids complicated upper operations. Experiments conducted on eight benchmark datasets demonstrate that our proposed archi- tecture promotes local feature driven models by a substantial margin and outperforms the previous best models in the fully-supervised setting.

In this work, we demonstrate the local feature ex- traction can be significantly enhanced with global information. Instead of traditionally exploiting deeper and complicated operations in upper neu- ral layers, our work innovatively provides another lightweight way for improving the ability of neu- ral model. Specifically, we propose a novel archi- tecture named Encoder1-Encoder2 with two In- teraction Modes for their interacting. The archi- tecture has high flexibility and our best models achieve new state-of-the-art performance in fully- supervised setting on all benchmark datasets. We also find that our architecture is insensitive to win- dow size and enjoy a better robustness. In fu- ture work, we plan to validate its effectiveness for multi-label classification. Besides, we are in- terested in incorporating more powerful unsuper- vised methods into our architecture.


### Latent-Variable Generative Models for Data-Efficient Text Classification

Generative classifiers offer potential advan- tages over their discriminative counterparts, namely in the areas of data efficiency, ro- bustness to data shift and adversarial exam- ples, and zero-shot learning (Ng and Jordan, 2002; Yogatama et al., 2017; Lewis and Fan, 2019). In this paper, we improve generative text classifiers by introducing discrete latent variables into the generative story, and explore several graphical model configurations. We parameterize the distributions using standard neural architectures used in conditional lan- guage modeling and perform learning by di- rectly maximizing the log marginal likelihood via gradient-based optimization, which avoids the need to do expectation-maximization. We empirically characterize the performance of our models on six text classification datasets. The choice of where to include the latent vari- able has a significant impact on performance, with the strongest results obtained when using the latent variable as an auxiliary condition- ing variable in the generation of the textual in- put. This model consistently outperforms both the generative and discriminative classifiers in small-data settings. We analyze our model by using it for controlled generation, finding that the latent variable captures interpretable prop- erties of the data, even with very small training sets.


### Text Level Graph Neural Network for Text Classification


Recently, researches have explored the graph neural network (GNN) techniques on text clas- sification, since GNN does well in handling complex structures and preserving global in- formation. However, previous methods based on GNN are mainly faced with the practical problems of fixed corpus level graph structure which do not support online testing and high memory consumption. To tackle the problems, we propose a new GNN based model that builds graphs for each input text with global parameters sharing instead of a single graph for the whole corpus. This method removes the burden of dependence between an individ- ual text and entire corpus which support online testing, but still preserve global information. Besides, we build graphs by much smaller windows in the text, which not only extract more local features but also significantly re- duce the edge numbers as well as memory con- sumption. Experiments show that our model outperforms existing models on several text classification datasets even with consuming less memory.


### Out-of-Domain Detection for Low-Resource Text Classification Tasks

Out-of-domain (OOD) detection for low- resource text classification is a realistic but understudied task. The goal is to detect the OOD cases with limited in-domain (ID) train- ing data, since we observe that training data is often insufficient in machine learning appli- cations. In this work, we propose an OOD- resistant Prototypical Network to tackle this zero-shot OOD detection and few-shot ID classification task. Evaluation on real-world datasets show that the proposed solution out- performs state-of-the-art methods in zero-shot OOD detection task, while maintaining a com- petitive performance on ID classification task.

### Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings

We propose a novel and simple method for semi-supervised text classification. The method stems from the hypothesis that a clas- sifier with pretrained word embeddings always outperforms the same classifier with randomly initialized word embeddings, as empirically observed in NLP tasks. Our method first builds two sets of classifiers as a form of model en- semble, and then initializes their word embed- dings differently: one using random, the other using pretrained word embeddings. We focus on different predictions between the two clas- sifiers on unlabeled data while following the self-training framework. We also use early- stopping in meta-epoch to improve the per- formance of our method. Our method, Delta- training, outperforms the self-training and the co-training framework in 4 different text clas- sification datasets, showing robustness against error accumulation.

In this paper, we propose a novel and simple approach for semi-supervised text classification. The method follows the conventional self-training framework, but focusing on different predictions between two sets of classifiers. Further, along with early-stopping in training processes and sim- ply adding all the unlabeled data with its pseudo- labels to training set, we can largely improve the model performance. Our framework, ∆-training, outperforms the conventional self-training and co- training framework in text classification tasks, showing robust performance against error accu- mulation.


### Cross-Cultural Transfer Learning for Text Classification

Large training datasets are required to achieve competitive performance in most natural lan- guage tasks. The acquisition process for these datasets is labor intensive, expensive, and time consuming. This process is also prone to hu- man errors. In this work, we show that cross- cultural differences can be harnessed for nat- ural language text classification. We present a transfer-learning framework that leverages widely-available unaligned bilingual corpora for classification tasks, using no task-specific data. Our empirical evaluation on two tasks – formality classification and sarcasm detec- tion – shows that the cross-cultural difference between German and American English, as manifested in product review text, can be ap- plied to achieve good performance for formal- ity classification, while the difference between Japanese and American English can be ap- plied to achieve good performance for sarcasm detection – both without any task-specific la- beled data.

### Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification

Supervised learning models often perform poorly at low-shot tasks, i.e. tasks for which little labeled data is available for training. One prominent approach for improving low-shot learning is to use unsupervised pre-trained neural models. Another approach is to ob- tain richer supervision by collecting anno- tator rationales (explanations supporting la- bel annotations). In this work, we com- bine these two approaches to improve low- shot text classification with two novel meth- ods: a simple bag-of-words embedding ap- proach; and a more complex context-aware method, based on the BERT model. In ex- periments with two English text classification datasets, we demonstrate substantial perfor- mance gains from combining pre-training with rationales. Furthermore, our investigation of a range of train-set sizes reveals that the sim- ple bag-of-words approach is the clear top performer when there are only a few dozen training instances or less, while more complex models, such as BERT or CNN, require more training data to shine.

In this work, we addressed an important chal- lenge in supervised machine learning, namely the dependency on large amounts of labeled training data. We demonstrated that substantial perfor- mance gains in low-shot text classification can be obtained by combining unsupervised pre-training with annotator rationales across various methods. To this end, we presented two novel methods that together provide strong results on a range of train set sizes. We performed experiments using various baselines, data sizes and ablations to help understand what works best for varying amounts of available training data. Most notably, we showed that simple bag-of-words methods with pre-trained word embeddings work best for very small train sets, while more complex methods based on pre-trained language models excel when more data is available.

### ProSeqo: Projection Sequence Networks for On-Device Text Classification

We propose a novel on-device sequence model for text classification using recurrent projec- tions. Our model ProSeqo uses dynamic re- current projections without the need to store or look up any pre-trained embeddings. This re- sults in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks. We conducted exhaustive evaluation on multi- ple text classification tasks. Results show that ProSeqo outperformed state-of-the-art neural and on-device approaches for short text clas- sification tasks such as dialog act and intent prediction. To the best of our knowledge, ProSeqo is the first on-device long text clas- sification neural model. It achieved compara- ble results to previous neural approaches for news article, answers and product categoriza- tion, while preserving small memory footprint and maintaining high accuracy.

### Induction Networks for Few-Shot Text Classification

Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes. In such challenging scenarios, recent studies have used meta-learning to simulate the few-shot task, in which new queries are compared to a small support set at the sample- wise level. However, this sample-wise com- parison may be severely disturbed by the var- ious expressions in the same class. Therefore, we should be able to learn a general repre- sentation of each class in the support set and then compare it to new queries. In this pa- per, we propose a novel Induction Network to learn such a generalized class-wise repre- sentation, by innovatively leveraging the dy- namic routing algorithm in meta-learning. In this way, we find the model is able to induce and generalize better. We evaluate the pro- posed model on a well-studied sentiment clas- sification dataset (English) and a real-world di- alogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, prov- ing the effectiveness of class-wise generaliza- tion in few-shot text classification.

In this paper, we propose the Induction Networks, a novel neural model for few-shot text classifica- tion. We propose to induce the class-level repre- sentations from support sets to deal with sample- wise diversity in few-shot learning tasks. The In- duction Module combines the dynamic routing al- gorithm with a meta-learning framework, and the routing mechanism makes our model more gen- eral to recognize unseen classes. The experiment results show that the proposed model outperforms the existing state-of-the-art few-shot text classi- fication models. We found that both the matrix transformation and routing procedure contribute consistently to the few-shot learning tasks.

### Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach

Zero-shot text classification (0SHOT-TC) is a challenging NLU problem to which little at- tention has been paid by the research com- munity. 0SHOT-TC aims to associate an ap- propriate label with a piece of text, irrespec- tive of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles study- ing 0SHOT-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0SHOT-TC. In addition, the chaotic experiments in literature make no uni- form comparison, which blurs the progress. This work benchmarks the 0SHOT-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0SHOT-TC relative to conceptually different and diverse aspects: the “topic” aspect includes “sports” and “politics” as labels; the “emotion” aspect includes “joy” and “anger”; the “situation” aspect includes “medical assistance” and “water shortage”. ii)We extend the existing evaluation setup (label-partially-unseen) – given a dataset, train on some labels, test on all labels – to include a more challenging yet realistic evaluation label-fully-unseen 0SHOT-TC (Chang et al.,2008), aiming at classifying text snippets without seeing task specific training data at all.iii) We unify the 0SHOT-TC of diverse aspects within a textual entailment formulation and study it this way.

### Topics to Avoid: Demoting Latent Confounds in Text Classification

Despite impressive performance on many text classification tasks, deep neural networks tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of native language identification. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the author’s native language is Swedish). We propose a method that represents the latent topical confounds and a model which “unlearns” confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the.

### Human-grounded Evaluations of Explanation Methods for Text Classification

Due to the black-box nature of deep learn- ing models, methods for explaining the mod- els’ results are crucial to gain trust from hu- mans and support collaboration between AIs and humans. In this paper, we consider sev- eral model-agnostic and model-specific expla- nation methods for CNNs for text classifica- tion and conduct three human-grounded eval- uations, focusing on different purposes of ex- planations: (1) revealing model behavior, (2) justifying model predictions, and (3) help- ing humans investigate uncertain predictions. The results highlight dissimilar qualities of the various explanation methods we consider and show the degree to which these methods could serve for each purpose.

We proposed three human tasks to evaluate local explanation methods for text classification. Us- ing the tasks in this paper, we experimented on 1D CNNs and found that (i) LIME is the most class discriminative method, justifying predictions with relevant evidence; (ii) LRP (N) works fairly well in helping humans investigate uncertain pre- dictions; (iii) using explanations to reveal model behavior is challenging, and none of the methods achieved impressive results; (iv) whenever using LRP and DeepLIFT, we should present to humans the most relevant words together with their con- texts and (v) the size of the DTs can also reflect the model complexity. Lastly, we consider evaluating on other datasets and other advanced architectures beneficial future work as it may reveal further in- teresting qualities of the explanation methods.

### Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification

However, these methods cannot capture the semantic relations (e.g., entity relations) and rely heavily on the number of training data. Clearly, lacking of train- ing data is still a key bottleneck that prohibits them from successful practical applications.

In this paper, we propose a novel heterogeneous graph neural network based method for semi- supervised short text classification, which takes full advantage of both limited labeled and large unlabeled data by information propagation. Partic- ularly, we first present a flexible HIN framework for modeling the short texts, which can integrate any additional information and capture their rich relations to address the semantic sparsity of short texts. Then, we propose a novel model HGAT to embed the HIN based on a dual-level attention mechanism including node-level and type-level attentions. HGAT considers the heterogeneity of various information types by projecting them into an implicit common space. Additionally, the dual- level attention captures the key information at multiple granularity levels and reduces the weights of noisy information. Extensive experimental re- sults demonstrated that our proposed model sig- nificantly outperforms the state-of-the-art methods across six benchmark datasets consistently.

### Adversarial Reprogramming of Text Classification Neural Networks

In this work, we develop methods to repurpose text classification neural networks for al- ternate tasks without modifying the network architecture or parameters. We propose a con- text based vocabulary remapping method that performs a computationally inexpensive input transformation to reprogram a victim classifi- cation model for a new set of sequences. We propose algorithms for training such an input transformation in both white box and black box settings where the adversary may or may not have access to the victim model’s archi- tecture and parameters. We demonstrate the application of our model and the vulnerability of neural networks by adversarially repurpos- ing various text-classification models includ- ing LSTM, bi-directional LSTM and CNN for alternate classification tasks.

In this paper, we extend adversarial reprogram- ming, a new class of adversarial attacks, to tar- get text classification neural networks. Our results demonstrate the effectiveness of such attacks in the more challenging black-box settings, posing them as a strong threat in real-world attack sce- narios. We demonstrate that neural networks can be effectively reprogrammed for alternate tasks, which were not originally intended by a service provider. Our proposed end-to-end approach can be used to further understand the vulnerabilities and blind spots of deep neural network based text classification systems. We recommend future work to study the scope of adversarial reprogram- ming for other NLP applications such as machine translation, text to speech synthesis and text to image synthesis where the input space is discrete. Furthermore, due to the threat presented by adver- sarial reprogramming, we recommend future work to study defenses against such attacks.

### Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification

Adversarial attacks against machine learning models have threatened various real-world ap- plications such as spam filtering and senti- ment analysis. In this paper, we propose a novel framework, learning to discriminate perturbations (DISP), to identify and adjust malicious perturbations, thereby blocking ad- versarial attacks for text classification models. To identify adversarial attacks, a perturbation discriminator validates how likely a token in the text is perturbed and provides a set of po- tential perturbations. For each potential per- turbation, an embedding estimator learns to re- store the embedding of the original word based on the context and a replacement token is cho- sen based on approximate kNN search. DISP can block adversarial attacks for any NLP model without modifying the model structure or training procedure. Extensive experiments on two benchmark datasets demonstrate that DISP significantly outperforms baseline meth- ods in blocking adversarial attacks for text classification. In addition, in-depth analysis shows the robustness of DISP across different situations.

### Sequential Learning of Convolutional Features for Effective Text Classification

Text classification has been one of the ma- jor problems in natural language processing. With the advent of deep learning, convolu- tional neural network (CNN) has been a pop- ular solution to this task. However, CNNs which were first proposed for images, face many crucial challenges in the context of text processing, namely in their elementary blocks: convolution filters and max pool- ing. These challenges have largely been over- looked by the most existing CNN models pro- posed for text classification. In this paper, we present an experimental study on the funda- mental blocks of CNNs in text categorization. Based on this critique, we propose Sequen- tial Convolutional Attentive Recurrent Net- work (SCARN). The proposed SCARN model utilizes both the advantages of recurrent and convolutional structures efficiently in compar- ison to previously proposed recurrent convo- lutional models. We test our model on differ- ent text classification datasets across tasks like sentiment analysis and question classification. Extensive experiments establish that SCARN outperforms other recurrent convolutional ar- chitectures with significantly less parameters. Furthermore, SCARN achieves better perfor- mance compared to equally large various deep CNN and LSTM architectures.

In this paper we present a critical study and viewpoint of CNNs, which even though are popularly used in text classification, details of it are of- ten overlooked. We find that convolutional fil- ters learn particularly in the context of sequen- tial information. But at the same time, they are good at learning higher level task-relevant fea- tures. On the other hand, we find max pooling to be very arbitrary in selection of crucial fea- tures and hence contributing minimal to the over- all task. We also find that the problems with in- put concatenation, as it imbalances the represen- tations because of difference in nature of distribu- tions. Based on our study we proposed SCARN, for effectively utilizing convolution and recurrent features for text classification. Our model beats other popular ways of combining recurrent and convolutional architectures with quite less number of parameters on various benchmark datasets. Our model also outperforms CNN and RNN architec- tures with equally deep or same number of param- eters.

### A Robust Self-Learning Framework for Cross-Lingual Text Classification

Based on massive amounts of data, recent pre-trained contextual representation models have made significant strides in advancing a num- ber of different English NLP tasks. However, for other languages, relevant training data may be lacking, while state-of-the-art deep learn- ing methods are known to be data-hungry. In this paper, we present an elegantly simple ro- bust self-learning framework to include unla- beled non-English samples in the fine-tuning process of pretrained multilingual represen- tation models. We leverage a multilingual model’s own predictions on unlabeled non- English data in order to obtain additional in- formation that can be used during further fine- tuning. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effec- tiveness on document and sentiment classifica- tion for a range of diverse languages.

### Metric Learning for Dynamic Text Classification

Traditional text classifiers are limited to predicting over a fixed set of labels. However, in many real-world applications the label set is frequently changing. For example, in intent classification, new intents may be added over time while others are removed.
We propose to address the problem of dynamic text classification by replacing the tra- ditional, fixed-size output layer with a learned, semantically meaningful metric space. Here the distances between textual inputs are opti- mized to perform nearest-neighbor classifica- tion across overlapping label sets. Changing the label set does not involve removing pa- rameters, but rather simply adding or remov- ing support points in the metric space. Then the learned metric can be fine-tuned with only a few additional training examples.
We demonstrate that this simple strategy is ro- bust to changes in the label space. Further- more, our results show that learning a non- Euclidean metric can improve performance in the low data regime, suggesting that fur- ther work on metric spaces may benefit low- resource research.

We propose a framework for dynamic text classification in which the label space is considered flex- ible and subject to frequent changes. We apply a metric learning method, namely prototypical net- work, and demonstrate its robustness for this task in a variety of data regimes. Motivated by the idea that new labels often originate from label splits, we extend prototypical networks to hyperbolic ge- ometry, derive expressions for hyperbolic proto- types, and demonstrate the effectiveness of our model in the low-resource setting. Our experimen- tal findings suggest that metric learning improves dynamic text classification models, and offer in- sights on how to combine low-resource training data from overlapping label sets. In the future we hope to explore other applications of metric learn- ing to low-resource research, possibly in combina- tion with explicit models for label entailment (tree learning, fuzzy sets), and/or Wasserstein distance.

### EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks

EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks

We have shown that simple data augmentation op- erations can boost performance on text classifi- cation tasks. Although improvement is at times marginal, EDA substantially boosts performance and reduces overfitting when training on smaller datasets. Continued work on this topic could ex- plore the theoretical underpinning of the EDA op- erations. We hope that EDA’s simplicity makes a compelling case for further thought.

### That’s So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using #petpeeve Tweets ∗
In this paper, we have presented a case study of the annoying behaviors using Twitter as a corpus. Our fine-grained visualization approach shows insights of different categories of these behaviors, with the geographical effects. We also show that linguistic cues are useful to categorize these behaviors automatically, and that using lexical and semantic embeddings as a data augmentation method significantly improves the performance.

http://www.cs.cmu.edu/˜yww/data/petpeeves.zip

### Dynamically Route Hierarchical Structure Representation to Attentive Capsule for Text Classification

Representation learning and feature aggregation are usually the two key intermediate steps in natural language processing. Despite deep neural networks have shown strong performance in the text classification task, they are unable to learn adaptive structure features automatically and lack of a method for fully utilizing the extrac- ted features. In this paper, we propose a novel architecture that dynamically routes hierarchical structure feature to attentive capsule, named HAC. Specifically, we first adopt intermediate informa- tion of a well-designed deep dilated CNN to form hierarchical structure features. Different levels of structure representations are corresponding to various linguistic units such as word, phrase and clause, respectively. Furthermore, we design a capsule module using dynamic routing and equip it with an attention mechanism. The attentive capsule implements an effective aggregation strategy for feature clustering and selection. Extensive results on eleven benchmark datasets demonstrate that the proposed model obtains competitive performance against several state-of-the-art baselines. Our code is available at https://github.com/zhengwsh/HAC.

In this paper, we propose a novel architecture that dynam- ically route hierarchical structure feature to attentive capsule for text classification. The main idea of the proposed model is to adaptively form multi-granularity structure representations of text and fully leverage the categorized abstract of features with attention. Experiments on various datasets demonstrate that the proposed approach outperforms competitors and achieve several state-of-the-art results. Ablation and visual- ization analyses also reveal the effectiveness of our model for structure representation learning and feature aggregation.

### Recurrent Neural Network for Text Classification with Hierarchical Multiscale Dense Connections

Text classification is a fundamental task in many Natural Language Processing applications. While recurrent neural networks have achieved great suc- cess in performing text classification, they fail to capture the hierarchical structure and long-term se- mantics dependency which are common features of text data. Inspired by the advent of the dense connection pattern in advanced convolutional neu- ral networks, we propose a simple yet effective recurrent architecture, named Hierarchical Mutis- cale Densely Connected RNNs (HM-DenseRNNs), which: 1) enables direct access to the hidden states of all preceding recurrent units via dense con- nections, and 2) organizes multiple densely con- nected recurrent units into a hierarchical multi- scale structure, where the layers are updated at dif- ferent scales. HM-DenseRNNs can effectively cap- ture long-term dependencies among words in long text data, and a dense recurrent block is further in- troduced to reduce the number of parameters and enhance training efficiency. We evaluate the per- formance of our proposed architecture on three text datasets and the results verify the advantages of HM-DenseRNN over the baseline methods in terms of the classification accuracy.


As future work, we plan to introduce additional decision variables to automatically tune the value of dense depth by the model itself. We also intend to apply the architecture to other RNN variants, and expect further performance gain with a better design of unit structure.

### A Span-based Joint Model for Opinion Target Extraction and Target Sentiment Classification

Target-Based Sentiment Analysis aims at extracting opinion targets and classifying the sentiment po- larities expressed on each target. Recently, token- based sequence tagging methods have been suc- cessfully applied to jointly solve the two tasks, which aims to predict a tag for each token. Since they do not treat a target containing several words as a whole, it might be difficult to make use of the global information to identify that opinion target, leading to incorrect extraction. Independently pre- dicting the sentiment for each token may also lead to sentiment inconsistency for different words in an opinion target. In this paper, inspired by span-based methods in NLP, we propose a simple and effective joint model to conduct extraction and classification at span level rather than token level. Our model first emulates spans with one or more tokens and learns their representation based on the tokens inside. And then, a span-aware attention mechanism is designed to compute the sentiment information towards each span. Extensive experiments on three benchmark datasets show that our model consistently outper- forms the state-of-the-art methods.


In this paper, we propose a span-based joint model for the complete TBSA task. Different from current token tagging based joint methods, our model can take advantage of the global information of a target. Furthermore, we present a span-aware attention mechanism to compute the sentiment information of the span. Calculating this information to each span rather than each word, our model avoid the sentiment inconsistency problem. We conduct experiments on three public datasets and the results show the effectiveness of our model.

### Accelerating Extreme Classification via Adaptive Feature Agglomeration

Extreme classification seeks to assign each data point, the most relevant labels from a universe of a million or more labels. This task is faced with the dual challenge of high precision and scalabil- ity, with millisecond level prediction times being a benchmark. We propose DEFRAG, an adaptive feature agglomeration technique to accelerate ex- treme classification algorithms. Despite past works on feature clustering and selection, DEFRAG dis- tinguishes itself in being able to scale to millions of features, and is especially beneficial when feature sets are sparse, which is typical of recommendation and multi-label datasets. The method comes with provable performance guarantees and performs ef- ficient task-driven agglomeration to reduce feature dimensionalities by an order of magnitude or more. Experiments show that DEFRAG can not only re- duce training and prediction times of several lead- ing extreme classification algorithms by as much as 40%, but also be used for feature reconstruction to address the problem of missing features, as well as offer superior coverage on rare labels.

### Adapting BERT for Target-Oriented Multimodal Sentiment Classification

As an important task in Sentiment Analysis, Target- oriented Sentiment Classification (TSC) aims to identify sentiment polarities over each opinion tar- get in a sentence. However, existing approaches to this task primarily rely on the textual content, ig- noring the other increasingly popular multimodal data sources (e.g., images), which can enhance the robustness of these text-based models. Mo- tivated by this observation and inspired by the recently proposed BERT architecture, we study Target-oriented Multimodal Sentiment Classifica- tion (TMSC) and propose a multimodal BERT ar- chitecture. To model intra-modality dynamics, we first apply BERT to obtain target-sensitive textual representations. We then borrow the idea from self- attention and design a target attention mechanism to perform target-image matching to derive target- sensitive visual representations. To model inter- modality dynamics, we further propose to stack a set of self-attention layers on top to capture multi- modal interactions. Experimental results show that our model can outperform several highly competi1 tive approaches for TSC and TMSC .

### Deep Correlated Predictive Subspace Learning for Incomplete Multi-View Semi-Supervised Classification

Incomplete view information often results in fail- ure cases of the conventional multi-view meth- ods. To address this problem, we propose a Deep Correlated Predictive Subspace Learning (D- CPSL) method for incomplete multi-view semi- supervised classification. Specifically, we integrate semi-supervised deep matrix factorization, corre- lated subspace learning, and multi-view label pre- diction into a unified framework to jointly learn the deep correlated predictive subspace and multi- view shared and private label predictors. DCP- SL is able to learn proper subspace representation that is suitable for class label prediction, which can further improve the performance of classification. Extensive experimental results on various practical datasets demonstrate that the proposed method performs favorably against the state-of-the-art methods.

We extract abstract and high-level multi-view repre- sentation by semi-supervised deep matrix factorization model. By encoding the label information into the rep- resentation, we can significantly improve the discrimi- nating power of the subspace representation.
We learn low-rank subspaces from deep matrix factor- ization model, where data correlation can be effectively extracted. Then the data correlation is further used to enhance the effectiveness of the learned subspace repre- sentation.
We propose to divide the label predictor into shared and private parts, which can effectively utilize multi- view complementary information and improve the per- formance of class label prediction.

In this paper, a deep correlated predictive subspace learn- ing method (DCPSL) is developed for incomplete multi- view semi-supervised classification. Our method is capa- ble of jointly leveraging the data correlations and multi-view complementary information, which is achieved by integrat- ing deep correlated predictive subspace learning and multi- view shared and private label prediction into a unified objec- tive function. Compared with the state-of-the-art multi-view semi-supervised learning methods, DCPSL can better han- dle the incomplete multi-view data and achieves competitive classification results on various practical datasets.

### Aspect-Based Sentiment Classification with Attentive Neural Turing Machines

Aspect-based sentiment classification aims to iden- tify sentiment polarity expressed towards a given opinion target in a sentence. The sentiment polarity of the target is not only highly determined by senti- ment semantic context but also correlated with the concerned opinion target. Existing works cannot effectively capture and store the inter-dependence between the opinion target and its context. To solve this issue, we propose a novel model of Attentive Neural Turing Machines (ANTM). Via interactive read-write operations between an external memory storage and a recurrent controller, ANTM can learn the dependable correlation of the opinion target to context and concentrate on crucial sentiment in- formation. Specifically, ANTM separates the in- formation of storage and computation, which ex- tends the capabilities of the controller to learn and store sequential features. The read and write op- erations enable ANTM to adaptively keep track of the interactive attention history between memory content and controller state. Moreover, we append target entity embeddings into both input and out- put of the controller in order to augment the in- tegration of target information. We evaluate our model on SemEval2014 dataset which contains re- views of Laptop and Restaurant domains and Twit- ter review dataset. Experimental results verify that our model achieves state-of-the-art performance on aspect-based sentiment classification.

In this paper, we proposed an model of Attentive Neural Machines (ANTM) for aspect term/opinion target level sentiment analysis. The motivation of ANTM is to deploy an external memory to separate storage information from computation in this way to extend the capabilities of neural networks. Exper- imental results show that ANTM performs superior perfor- mance compared with these baselines, especially can boost the efficiency of dispose of the long-sequential-distance text. ANTM also proves the extended memory with drawing sup- port from sequence output of pre-trained BERT model per- forms better in the small-scale corpuses. Potential future plan is to demonstrate the stability and superiority of ANTM ap- plying to longer sequences for other sentiment analysis tasks.

### Deep Mask Memory Network with Semantic Dependency and Context Moment for Aspect Level Sentiment Classification

Aspect level sentiment classification aims at iden- tifying the sentiment of each aspect term in a sen- tence. Deep memory networks often use location information between context word and aspect to generate the memory. Although improved results are achieved, the relation information among as- pects in the same sentence is ignored and the word location can’t bring enough and accurate informa- tion for the analysis on the aspect sentiment. In this paper, we propose a novel framework for aspect level sentiment classification, deep mask mem- ory network with semantic dependency and con- text moment (DMMN-SDCM), which integrates semantic parsing information of the aspect and the inter-aspect relation information into deep memory network. With the designed attention mechanism based on semantic dependency information, differ- ent parts of the context memory in different compu- tational layers are selected and useful inter-aspect information in the same sentence is exploited for the desired aspect. To make full use of the inter- aspect relation information, we also jointly learn a context moment learning task, which aims to learn the sentiment distribution of the entire sentence for providing a background for the desired aspect. We examined the merit of our model on SemEval 2014 Datasets, and the experimental results show that our model achieves a state-of-the-art performance.

In this paper, we design a deep mask memory network with semantic dependency and context moment (DMMN-SDCM) which integrates semantic parsing information and context moment learning into deep memory network for the first time. With the semantic dependency, we proposed a more discrim- inative attention scheme, which effectively selects different parts of the context memory for different computational lay- ers, and presented an effective method to model the inter- aspect relation. The sentiment distribution of the entire sen- tence is also encoded by using a context moment for the first time, which guides the deep memory network to learn an ef- fective feature. We have conducted extensive experiments on SemEval 2014 review datasets, and the experiment results clearly show that our model performance is state-of-the-art.

### Deterministic Routing between Layout Abstractions for Multi-Scale classification of Visually Rich Documents

Classifying heterogeneous visually rich documents is a challenging task. Difficulty of this task in- creases even more if the maximum allowed infer- ence turnaround time is constrained by a thresh- old. The increased overhead in inference cost, com- pared to the limited gain in classification capabil- ities make current multi-scale approaches infeasi- ble in such scenarios. There are two major con- tributions of this work. First, we propose a spa- tial pyramid model to extract highly discrimina- tive multi-scale feature descriptors from a visually rich document by leveraging the inherent hierarchy of its layout. Second, we propose a determinis- tic routing scheme for accelerating end-to-end in- ference by utilizing the spatial pyramid model. A depth-wise separable multi-column convolutional network is developed to enable our method. We evaluated the proposed approach on four publicly available, benchmark datasets of visually rich doc- uments. Results suggest that our proposed ap- proach demonstrates robust performance compared to the state-of-the-art methods in both classification accuracy and total inference turnaround.

### Exploiting Interaction Links for Node Classification with Deep Graph Neural Networks

Node classification is an important problem in re- lational machine learning. However, in scenarios where graph edges represent interactions among the entities (e.g., over time), the majority of cur- rent methods either summarize the interaction in- formation into link weights or aggregate the links to produce a static graph. In this paper, we propose a neural network architecture that jointly captures both temporal and static interaction patterns, which we call Temporal-Static-Graph-Net (TSGNet). Our key insight is that leveraging both a static neigh- bor encoder, which can learn aggregate neighbor patterns, and a graph neural network-based recur- rent unit, which can capture complex interaction patterns, improve the performance of node clas- sification. In our experiments on node classifica- tion tasks, TSGNet produces significant gains com- pared to state-of-the-art methods—reducing clas- sification error up to 24% and an average of 10% compared to the best competitor on four real-world networks and one synthetic dataset.

### Fast and Accurate Classification with a Multi-Spike Learning Algorithm for Spiking Neurons

The formulation of efficient supervised learning algorithms for spiking neurons is complicated and remains challenging. Most existing learning methods with the precisely firing times of spikes often result in relatively low efficiency and poor robustness to noise. To address these limitations, we propose a simple and effective multi-spike learning rule to train neurons to match their output spike number with a desired one. The proposed method will quickly find a local maximum value (directly relat- ed to the embedded feature) as the relevant signal for synaptic updates based on membrane potential trace of a neuron, and constructs an error function defined as the difference between the local maxi- mum membrane potential and the firing threshold. With the presented rule, a single neuron can be trained to learn multi-category tasks, and can suc- cessfully mitigate the impact of the input noise and discover embedded features. Experimental results show the proposed algorithm has higher precision, lower computation cost, and better noise robustness than current state-of-the-art learning methods under a wide range of learning tasks.


### Graph Neural Networks with Convolutional ARMA Filters

Recent graph neural networks implement convo- lutional layers based on polynomial filters oper- ating in the spectral domain. In this paper, we propose a novel graph convolutional layer based on auto-regressive moving average (ARMA) fil- ters that, compared to the polynomial ones, pro- vide a more flexible response thanks to a rich transfer function that accounts for the concept of state. We implement the ARMA filter with a recursive and distributed formulation, obtain- ing a convolutional layer that is efficient to train, it is localized in the node space and can be ap- plied to graphs with different topologies. In or- der to learn more abstract and compressed rep- resentations in deeper layers of the network, we alternate pooling operations based on node deci- mation with convolutions on coarsened versions of the original graph. We consider three major graph inference problems: semi-supervised node classification, graph classification, and graph sig- nal classification. Results show that the proposed graph neural network with ARMA filters outper- form those based on polynomial filters and sets the new state-of-the-art in several tasks.

We proposed a recursive formulation of the ARMA graph convolutional layer, which allows for a fast and distributed GNN implementation that exploits efficient sparse tensor operations to perform graph convolutions with the Lapla- cians. The proposed ARMA layer outperformed existing convolutional layers based on polynomial filters on all clas- sification tasks on graph data taken into account. To build a deep GNN, we used a pooling operation based on node decimation, which achieves superior performance on real- world graphs with irregular topology and faster training time compared to node pooling based on graph clustering.


### Hierarchical Inter-Attention Network for Document Classification with Multi-Task Learning

Document classification is an essential task in many real world applications. Existing approaches adopt both text semantics and document structure to ob- tain the document representation. However, these models usually require a large collection of anno- tated training instances, which are not always fea- sible, especially in low-resource settings. In this paper, we propose a multi-task learning framework to jointly train multiple related document classifica- tion tasks. We devise a hierarchical architecture to make use of the shared knowledge from all tasks to enhance the document representation of each task. We further propose an inter-attention approach to improve the task-specific modeling of documents with global information. Experimental results on 15 public datasets demonstrate the benefits of our proposed model.

### Multi-Domain Sentiment Classification Based on Domain-Aware Embedding and Attention
Sentiment classification is a fundamental task in NLP. However, as revealed by many researches, sentiment classification models are highly domain- dependent. It is worth investigating to leverage data from different domains to improve the clas- sification performance in each domain. In this work, we propose a novel completely-shared multi- domain neural sentiment classification model to learn domain-aware word embeddings and make use of domain-aware attention mechanism. Our model first utilizes BiLSTM for domain classi- fication and extracts domain-specific features for words, which are then combined with general word embeddings to form domain-aware word embed- dings. Domain-aware word embeddings are fed into another BiLSTM to extract sentence features. The domain-aware attention mechanism is used for selecting significant features, by using the domain- aware sentence representation as the query vec- tor. Evaluation results on public datasets with 16 different domains demonstrate the efficacy of our proposed model. Further experiments show the generalization ability and the transferability of our model.

In this paper, we propose a novel completely-shared neural model to make use of different training data across all do- mains. Our model builds domain-aware word embedding to express domain and context information for words, and pro- poses domain-aware attention mechanism to focus on more significant words in the text. Experiments on multi-domain sentiment classification and cross-domain sentiment classifi- cation on 16 different domains demonstrate the effectiveness and advantages of our proposed model.

### Spatio-Temporal Attentive RNN for Node Classification in Temporal Attributed Graphs

Node classification in graph-structured data aims to classify the nodes where labels are only available for a subset of nodes. This problem has attracted considerable research efforts in recent years. In real-world applications, both graph topology and node attributes evolve over time. Existing tech- niques, however, mainly focus on static graphs and lack the capability to simultaneously learn both temporal and spatial/structural features. Node clas- sification in temporal attributed graphs is challeng- ing for two major aspects. First, effectively mod- eling the spatio-temporal contextual information is hard. Second, as temporal and spatial dimen- sions are entangled, to learn the feature represen- tation of one target node, it’s desirable and chal- lenging to differentiate the relative importance of different factors, such as different neighbors and time periods. In this paper, we propose STAR, a spatio-temporal attentive recurrent network model, to deal with the above challenges. STAR extracts the vector representation of neighborhood by sam- pling and aggregating local neighbor nodes. It further feeds both the neighborhood representation and node attributes into a gated recurrent unit net- work to jointly learn the spatio-temporal contextual information. On top of that, we take advantage of the dual attention mechanism to perform a thorough analysis on the model interpretability. Extensive experiments on real datasets demonstrate the effec- tiveness of the STAR model.

In this paper, we propose a novel method, STAR, for node classification in temporal attributed graphs. STAR consists of a spatio-temporal GRU and a dual attention module. By feeding the sequential node attributes and the neighborhood representations into the GRU, the temporal features of at- tributes evolution and the spatial information of the node’s local neighborhood can be effectively modeled respectively. A dual attention mechanism is developed to perform a thor- ough analysis on the interpretability of STAR. And it helps STAR detect the time steps and the node neighbors that are more important for the classification. Extensive experimental results demonstrate the effectiveness of STAR.





